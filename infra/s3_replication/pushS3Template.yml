AWSTemplateFormatVersion: 2010-09-09
Transform:
- AWS::Serverless-2016-10-31

Parameters:
  S3DestinationBucket:
    Type: String
    Description: S3 Destination bucket that has allowed access to the calling role.
  S3SourceBucket:
    Type: String
    Description: S3 Source bucket that is in this account
  IAMRoleToAssume:
    Type: String
    Description: IAM Role to assume from the other account.
  ListOfGithubOrgs:
    Type: String
    Description: Comma separated string of Github organizations to use.
  LambdaRoleName:
    Type: String
    Description: Destination Account to push files to.

Resources:
  #Roles
  LambdaExecutionRole:
    Description: Creating service role in IAM for AWS Lambda
    Type: AWS::IAM::Role
    Properties:
      RoleName: {Ref: LambdaRoleName}
      AssumeRolePolicyDocument:
        Statement:
        - Effect: Allow
          Principal:
            Service: [lambda.amazonaws.com]
          Action: 
            - sts:AssumeRole
      # Path: /
      Policies:
      - PolicyDocument:
          Statement:
          - Action: ['cloudwatch:*', 'logs:*', 'sts:assumeRole', 'lambda:InvokeFunction']
            Effect: Allow
            Resource: '*'
          Version: '2012-10-17'
        PolicyName: lambdaPolicyAssumeRole
  S3ReplicateJob:
    Type: AWS::Events::Rule
    Properties:
      Description: "Defines event rule to run the S3 replication job."
      ScheduleExpression: "cron(00 23 * * ? *)"
      State: "ENABLED"
      Targets:
        -
          Arn:
            Fn::GetAtt:
            - LambdaS3Replicator
            - Arn
          Id: "LambdaS3Replicator"
   #Permission for event triggering lambda
  LambdaEventTriggerPermission:
    Type: AWS::Lambda::Permission
    Properties: 
      Action: lambda:InvokeFunction
      FunctionName:
        Fn::GetAtt: 
          - LambdaS3Replicator
          - Arn
      Principal: "events.amazonaws.com"
      SourceArn: 
        Fn::GetAtt: 
          - S3ReplicateJob
          - Arn
  LambdaS3Replicator:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.replicate
      Runtime: python3.6
      Timeout: 15
      Environment:
        Variables:
          SourceS3Bucket: {Ref: S3SourceBucket}
          DestinationS3Bucket: {Ref: S3DestinationBucket}
          iamRole: {Ref: IAMRoleToAssume}
          GithubOrgs: {Ref: ListOfGithubOrgs}
      Role:
        Fn::GetAtt:
        - LambdaExecutionRole
        - Arn
      Code:
        ZipFile: |
          import boto3
          import os
          import datetime
          import logging
          from botocore.exceptions import ClientError

          BACKFILL_EVENT_KEY = 'backfill'
          CONTENTS_KEY = 'Contents'

          def replicate(event, context):
              today = datetime.datetime.now().strftime("%Y-%m-%d")
              dates = [today]
              if BACKFILL_EVENT_KEY in event.keys() :
                  dates = event[BACKFILL_EVENT_KEY]

              ORGS = os.environ['GithubOrgs'].split(',')
              logger = logging.getLogger()
              logger.setLevel(logging.INFO) #Todo bump up to warn

              IAM_ROLE_ASSUME = os.environ['iamRole']
              destinationS3Bucket = os.environ['DestinationS3Bucket']
              sourceS3Bucket = os.environ['SourceS3Bucket']
              try :
                  # Get our info from the assumed role
                  sts = boto3.client('sts')
                  stsAssumedCredentials = sts.assume_role(
                      RoleArn=IAM_ROLE_ASSUME,
                      RoleSessionName='assumeForS3Copy'
                  )['Credentials'] 

                  accessId = stsAssumedCredentials['AccessKeyId']
                  secretId = stsAssumedCredentials['SecretAccessKey']
                  sessionToken = stsAssumedCredentials['SessionToken']

                  #Now Make our call to S3 local and S3 away.
                  s3Client = boto3.client(
                      's3',
                      aws_access_key_id=accessId,
                      aws_secret_access_key=secretId,
                      aws_session_token=sessionToken,
                  )

                  KEY_STRUCTURE_PREFIX = '{date}/traffic/{parentRepo}'#-{name}-traffic-{timestamp}.json

                  for date in dates:
                      for repo in ORGS :
                          allObjectsWithPrefix = s3Client.list_objects_v2(
                              Bucket=sourceS3Bucket,
                              Prefix=KEY_STRUCTURE_PREFIX.format(date=date, parentRepo=repo)
                          )
                          # jsonObjectsWithPrefix = json.loads(allObjectsWithPrefix)
                          if CONTENTS_KEY in allObjectsWithPrefix.keys():
                              for record in allObjectsWithPrefix[CONTENTS_KEY]:
                                  key = record['Key']
                                  # For each record, copy
                                  copySource = {
                                      'Bucket':sourceS3Bucket,
                                      'Key': key
                                  }
                                  s3Client.copy(copySource, destinationS3Bucket, key)
                          else:
                              logging.warn(f'Failed to find contents for repo {repo} on {date}')
              except ClientError as e: 
                  logging.critical(e)
                  logging.error(f'Failed to copy on {today}');
                  return {
                      'statusCode': 500,
                      'headers': {"Content-Type": "text/plain"},
                      'body': f'failed on {today}',
                  }
              return {
                  'statusCode': 200,
                  'headers': {"Content-Type": "text/plain"},
                  'body': f'Succeeded on {today}',
              }
